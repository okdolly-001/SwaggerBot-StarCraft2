#Implement a Deep Q learning agent on CartPole task in PyTorch 
# Using the tutorial https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
# This helped me understand how to use replays in RL. 

#Goal — Balance the pole on top of a moving cart
#State — angle, angular speed, position, horizontal velocity
#Actions — horizontal force to the cart
#Reward — 1 at each time step if the pole is upright

#Import needed packages 
import gym #for the environment 
# the gym library provides an easy to use suite of RL tasks
import math 
import random 
import numpy as np 
import matplotlib
import matplotlib.pyplot as plt 
from collections import namedtuple 
from itertools import count 
from PIL import Image 

import torch
import torch.nn as nn #for neural net 
import torch.optim as optim # for optimization 
import torch.nn.functional as F 
import torchvision.transforms as T


env = gym.make('CartPole-v0').unwrapped 

#set up matplotlib
is_ipython = 'inline' in matplotlib.get_backend()
if is_ipython:
	from Ipython import display 

plt.ion()

#if gpu is to be used 
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#Reply memory stores the transitions that the agent observes which we can use later 


#Transition is a namedtuple representing a single transition in our env 
Transition = namedtuple('Transition', 
							('state', 'action', 'next_state', 'reward'))

#ReplayMemory - Its a buffer that holds transitons observed recently 
class ReplayMemory(object):

	def __init__(self, capacity):
		self.capacity = capacity 
		self.memory = []
		self.position = 0 

	def push(self, *args): #Saves a transition 
		if len(self.memory) < self.capacity:
			self.memory.append(None)
		self.memory[self.position] = Transition(*args)
		self.position = (self.position + 1) % self.capacity 

	#For selecting random batch of transitions for trainng 
	#So that transitions that build up a batch are decorrelated 
	def sample(self, batch_size): 
		return random.sample(self.memory, batch_size) 

	def __len__(self):
		return len(self.memory)


## Deep Q Network 
## Develop a convolutional neural net takes previous and current screen patch 
# This is because of Markov decision problem 

class DQN(nn.Module): ##nn.Module is base class for all NN modules 

	def __init__(self):
		super(DQN, self).__init__() #Gain access to inherited methods

		#In conv: kernel_size is filter size, stride is filter movement or step 
		# kernel_size refers to how large a window we would like to slide over the previous layer.
		# Stride refers to how many pixels we want to skip as we slide the window across the layer
		#Batch Normalization: normalizes output from activation function 
		self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride =2)# 3 is in channel, 16 is out 
		self.bn1 = nn.BatchNorm2d(16) #here 16 is numer of features 
		self.conv2 = nn.Conv2d(16,32, kernel_size=5, stride = 2)
		self.bn2 = nn.BatchNorm2d(32)
		self.conv3 = nn.Conv2d(32,32, kernel_size=5, stride = 2)
		self.bn3 = nn.BatchNorm2d(32)
		self.head = nn.Linear(448, 2) #Applies Linear transformation to incoming data 

	def forward(self, x):
		x = F.relu(self.bn1(self.conv1(x)))
		x = F.relu(self.bn2(self.conv2(x)))
		x = F.relu(self.bn3(self.conv3(x)))
		return self.head(x.view(x.size(0), -1))



##Input extract 
## we will have to extract and process rendered images from the environment. 

resize = T.Compose([T.ToPILImage(), 
					T.Resize(40, interpolation= Image.CUBIC),
					T.ToTensor()])

# This is based on the code from gym 
screen_width = 600 

def get_cart_location():
	world_width = env.x_threshold * 2
	scale = screen_width / world_width
	return int(env.state[0] * scale + screen_width /2.0) #Middle of Cart 

def get_screen():
    screen = env.render(mode='rgb_array').transpose(
        (2, 0, 1))  # transpose into torch order (CHW)
    # Strip off the top and bottom of the screen
    screen = screen[:, 160:320]
    view_width = 320
    cart_location = get_cart_location()
    if cart_location < view_width // 2:
        slice_range = slice(view_width)
    elif cart_location > (screen_width - view_width // 2):
        slice_range = slice(-view_width, None)
    else:
        slice_range = slice(cart_location - view_width // 2,
                            cart_location + view_width // 2)
    # Strip off the edges, so that we have a square image centered on a cart
    screen = screen[:, :, slice_range]
    # Convert to float, rescare, convert to torch tensor
    # (this doesn't require a copy)
    screen = np.ascontiguousarray(screen, dtype=np.float32) / 255
    screen = torch.from_numpy(screen)
    # Resize, and add a batch dimension (BCHW)
    return resize(screen).unsqueeze(0).to(device)


env.reset()
plt.figure()
plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(),
           interpolation='none')
plt.title('Example extracted screen')
plt.show()


###Training Setup 

BATCH_SIZE = 128 # Drawing batches of replay
GAMMA = 0.999
EPS_START = 0.9  #The probability of choosing the action
EPS_END = 0.05   # Will decay exponentially at this probability 
EPS_DECAY = 200  #Controls the rate of decay 
TARGET_UPDATE = 10 

policy_net = DQN().to(device)
target_net = DQN().to(device)
target_net.load_state_dict(policy_net.state_dict())
target_net.eval()

optimizer = optim.RMSprop(policy_net.parameters())
memory = ReplayMemory(10000)


steps_done = 0

#select_action will selct an action accordingly to an epsilon greedy policy. 
def select_action(state):
    global steps_done
    sample = random.random()
    eps_threshold = EPS_END + (EPS_START - EPS_END) * \
        math.exp(-1. * steps_done / EPS_DECAY)
    steps_done += 1
    if sample > eps_threshold:
        with torch.no_grad():
            return policy_net(state).max(1)[1].view(1, 1)
    else:
        return torch.tensor([[random.randrange(2)]], device=device, dtype=torch.long)



episode_durations =[]

#plot_duration: a helper for plotting the durations of episodes, along with an average
#over the last 100 episodes. 
#This will help visualize the training process 

def plot_durations():
    plt.figure(2)
    plt.clf()
    durations_t = torch.tensor(episode_durations, dtype=torch.float)
    plt.title('Training...')
    plt.xlabel('Episode')
    plt.ylabel('Duration')
    plt.plot(durations_t.numpy())
    # Take 100 episode averages and plot them too
    if len(durations_t) >= 100:
        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)
        means = torch.cat((torch.zeros(99), means))
        plt.plot(means.numpy())

    plt.pause(0.001)  # pause a bit so that plots are updated
    if is_ipython:
        display.clear_output(wait=True)
        display.display(plt.gcf())




## Training Loop: the code for training our model 
# optimize_model: performs a single step of the optimization.
def optimize_model():
    if len(memory) < BATCH_SIZE:
        return
    transitions = memory.sample(BATCH_SIZE)
    # Transpose the batch.
    batch = Transition(*zip(*transitions))

    # Compute a mask of non-final states and concatenate the batch elements
    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,
                                          batch.next_state)), device=device, dtype=torch.uint8)
    non_final_next_states = torch.cat([s for s in batch.next_state
                                                if s is not None])

    state_batch = torch.cat(batch.state)
    action_batch = torch.cat(batch.action)
    reward_batch = torch.cat(batch.reward)

    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the
    # columns of actions taken
    state_action_values = policy_net(state_batch).gather(1, action_batch)

    # Compute V(s_{t+1}) for all next states.
    next_state_values = torch.zeros(BATCH_SIZE, device = device)
    next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0].detach()

      # Compute the expected Q values
    expected_state_action_values = (next_state_values * GAMMA) + reward_batch

    # Compute Huber loss
    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))

    # Optimize the model
    optimizer.zero_grad()
    loss.backward()
    for param in policy_net.parameters():
        param.grad.data.clamp_(-1, 1)
    optimizer.step()


num_episodes = 50
for i_episode in range(num_episodes):
    # Initialize the environment and state
    env.reset()
    last_screen = get_screen()
    current_screen = get_screen()
    state = current_screen - last_screen
    for t in count():
        # Select and perform an action
        action = select_action(state)
        _, reward, done, _ = env.step(action.item())
        reward = torch.tensor([reward], device=device)

        # Observe new state
        last_screen = current_screen
        current_screen = get_screen()
        if not done:
            next_state = current_screen - last_screen
        else:
            next_state = None

        # Store the transition in memory
        memory.push(state, action, next_state, reward)

        # Move to the next state
        state = next_state

        # Perform one step of the optimization (on the target network)
        optimize_model()
        if done:
            episode_durations.append(t + 1)
            plot_durations()
            break
    # Update the target network
    if i_episode % TARGET_UPDATE == 0:
        target_net.load_state_dict(policy_net.state_dict())

print('Complete')
env.render()
env.close()
plt.ioff()
plt.show()



